// ============================================================================
// AILANG Logging Library
// High-Performance Structured Logging with Cache-Aware Async Architecture
// Critical for AI-Generated Code Debugging and System Monitoring
// ============================================================================

// Cache-Optimized Memory Pools for High-Performance Logging Operations
Pool.Logging.Messages = FixedPool {
    "log_entries": ElementType-LogEntry, MaximumLength-10000000,
    "log_contexts": ElementType-LogContext, MaximumLength-1000000,
    "structured_data": ElementType-Address, MaximumLength-10000000,
    "cache_alignment": Initialize-64, CanChange-False
}

Pool.Logging.AsyncBuffers = RingPool {
    "ring_buffer": ElementType-Byte, MaximumLength-1073741824,  // 1GB ring buffer
    "message_queue": ElementType-LogMessage, MaximumLength-1000000,
    "batch_buffers": ElementType-Address, MaximumLength-10000,
    "cache_policy": Initialize-"L2", CanChange-False  // L2 for async processing
}

Pool.Logging.Formatters = TemporalPool {
    "format_templates": ElementType-FormatTemplate, MaximumLength-10000,
    "formatted_messages": ElementType-Text, MaximumLength-1000000,
    "color_sequences": ElementType-Text, MaximumLength-1000,
    "lifetime": Initialize-"session_scope", CanChange-False
}

Pool.Logging.Outputs = FixedPool {
    "file_handles": ElementType-FileHandle, MaximumLength-1000,
    "network_sockets": ElementType-SocketHandle, MaximumLength-100,
    "console_handles": ElementType-ConsoleHandle, MaximumLength-10,
    "journal_handles": ElementType-JournalHandle, MaximumLength-10
}

Pool.Logging.Filtering = DynamicPool {
    "filter_chains": ElementType-FilterChain, MaximumLength-10000,
    "log_levels": ElementType-LogLevel, MaximumLength-1000000,
    "namespace_filters": ElementType-NamespaceFilter, MaximumLength-100000,
    "sampling_state": ElementType-Address, CanChange-True
}

Pool.Logging.Correlation = TemporalPool {
    "correlation_ids": ElementType-Text, MaximumLength-1000000,
    "trace_contexts": ElementType-TraceContext, MaximumLength-100000,
    "request_contexts": ElementType-RequestContext, MaximumLength-100000,
    "lifetime": Initialize-"request_scope", CanChange-False
}

Pool.Logging.Metrics = FixedPool {
    "performance_counters": ElementType-PerformanceCounter, MaximumLength-100000,
    "log_statistics": ElementType-LogStats, MaximumLength-1000,
    "sampling_metrics": ElementType-SamplingMetrics, MaximumLength-10000,
    "cache_policy": Initialize-"L1", CanChange-False  // Fast access for metrics
}

// ============================================================================
// Core Logging Data Types
// ============================================================================

LogLevel = Record {
    name: Text,  // "TRACE", "DEBUG", "INFO", "WARN", "ERROR", "FATAL"
    numeric_value: Integer,  // 0-50 for ordering
    color_code: Text,
    should_flush: Boolean
}

LogEntry = Record {
    level: LogLevel,
    message: Text,
    structured_data: Map[Text, Any],
    timestamp_ns: UInt64,
    thread_id: UInt64,
    logger_name: Text,
    source_file: Text,
    source_line: Integer,
    correlation_id: OptionalType[Text],
    trace_id: OptionalType[Text],
    span_id: OptionalType[Text]
}

LogMessage = Record {
    entry: LogEntry,
    formatted_message: Text,
    target_outputs: Array[Text],  // Output destination names
    processing_priority: Integer,
    batch_id: UInt64
}

LogContext = Record {
    logger_name: Text,
    namespace: Text,
    level_filter: LogLevel,
    outputs: Array[LogOutput],
    formatters: Array[LogFormatter],
    filters: Array[LogFilter],
    async_enabled: Boolean,
    buffer_size: Integer
}

Logger = Record {
    name: Text,
    context: LogContext,
    parent: OptionalType[Logger],
    children: Array[Logger],
    statistics: LogStats,
    created_at: UInt64
}

// ============================================================================
// Output Target Types
// ============================================================================

LogOutput = Record {
    name: Text,
    type: Text,  // "console", "file", "network", "journal", "memory"
    configuration: Map[Text, Any],
    formatter: LogFormatter,
    level_filter: LogLevel,
    active: Boolean
}

ConsoleHandle = Record {
    output_stream: Text,  // "stdout", "stderr"
    color_enabled: Boolean,
    encoding: Text,
    buffer_mode: Text  // "line", "full", "none"
}

FileHandle = Record {
    file_path: Text,
    file_descriptor: Integer,
    rotation_policy: FileRotationPolicy,
    buffer_size: Integer,
    sync_frequency: Integer,  // Sync every N writes
    creation_time: UInt64
}

FileRotationPolicy = Record {
    type: Text,  // "size", "time", "daily", "weekly"
    max_size_bytes: UInt64,
    max_files: Integer,
    rotation_time: OptionalType[Text],  // "23:59", "hourly"
    compress_old: Boolean
}

SocketHandle = Record {
    protocol: Text,  // "tcp", "udp", "unix"
    address: Text,
    port: Integer,
    connection_timeout: Integer,
    retry_policy: NetworkRetryPolicy,
    tls_enabled: Boolean
}

NetworkRetryPolicy = Record {
    max_retries: Integer,
    initial_delay_ms: Integer,
    backoff_multiplier: FloatingPoint,
    max_delay_ms: Integer
}

JournalHandle = Record {
    service_name: Text,
    facility: Text,  // "daemon", "user", "local0-7"
    identifier: Text,
    socket_path: Text
}

// ============================================================================
// Formatting and Filtering
// ============================================================================

LogFormatter = Record {
    name: Text,
    type: Text,  // "simple", "json", "structured", "custom"
    template: Text,
    timestamp_format: Text,
    include_source: Boolean,
    include_thread: Boolean,
    custom_fields: Map[Text, Text]
}

FormatTemplate = Record {
    pattern: Text,
    compiled_regex: Address,  // Compiled regex for performance
    field_extractors: Array[FieldExtractor],
    color_mappings: Map[Text, Text]
}

FieldExtractor = Record {
    field_name: Text,
    extraction_function: Function,
    format_specification: Text
}

LogFilter = Record {
    name: Text,
    type: Text,  // "level", "namespace", "sampling", "rate_limit", "custom"
    configuration: Map[Text, Any],
    filter_function: Function,
    pass_through: Boolean
}

FilterChain = Record {
    filters: Array[LogFilter],
    default_action: Text,  // "pass", "block"
    cache_decisions: Boolean,
    decision_cache: Map[Text, Boolean]
}

NamespaceFilter = Record {
    namespace_pattern: Text,
    compiled_pattern: Address,
    action: Text,  // "include", "exclude"
    level_override: OptionalType[LogLevel]
}

// ============================================================================
// Correlation and Tracing
// ============================================================================

TraceContext = Record {
    trace_id: Text,
    span_id: Text,
    parent_span_id: OptionalType[Text],
    baggage: Map[Text, Text],
    sampling_decision: Boolean,
    created_at: UInt64
}

RequestContext = Record {
    request_id: Text,
    user_id: OptionalType[Text],
    session_id: OptionalType[Text],
    operation_name: Text,
    tags: Map[Text, Text],
    start_time: UInt64
}

// ============================================================================
// Performance and Statistics
// ============================================================================

LogStats = Record {
    messages_logged: UInt64,
    messages_by_level: Map[Text, UInt64],
    bytes_written: UInt64,
    cache_hits: UInt64,
    cache_misses: UInt64,
    async_queue_depth: Integer,
    dropped_messages: UInt64,
    last_updated: UInt64
}

PerformanceCounter = Record {
    name: Text,
    counter_type: Text,  // "gauge", "counter", "histogram"
    current_value: FloatingPoint,
    total_samples: UInt64,
    min_value: FloatingPoint,
    max_value: FloatingPoint,
    last_updated: UInt64
}

SamplingMetrics = Record {
    total_candidates: UInt64,
    sampled_count: UInt64,
    sampling_rate: FloatingPoint,
    adaptive_rate: Boolean,
    last_adjustment: UInt64
}

// ============================================================================
// Core Logging API
// ============================================================================

Function.Logging.GetLogger {
    Input: (
        name: Text,
        level: Text = "INFO",
        async_enabled: Boolean = True
    )
    Output: Logger
    Body: {
        // Check if logger already exists
        existing_logger = Logging.FindExistingLogger(name)
        IfCondition NotEqual(existing_logger, Null) ThenBlock {
            ReturnValue(existing_logger)
        }
        
        Pool.Logging.Messages.Allocate(logger)
        Pool.Logging.Messages.Allocate(context)
        
        // Set up log context
        context.logger_name = name
        context.namespace = Logging.ExtractNamespace(name)
        context.level_filter = Logging.ParseLogLevel(level)
        context.outputs = Array.Create()
        context.formatters = Array.Create()
        context.filters = Array.Create()
        context.async_enabled = async_enabled
        context.buffer_size = 8192  // 8KB default buffer
        
        // Initialize logger
        logger.name = name
        logger.context = context
        logger.parent = Logging.FindParentLogger(name)
        logger.children = Array.Create()
        logger.statistics = Logging.CreateEmptyStats()
        logger.created_at = Time.GetUnixTimestampNs()
        
        // Add default console output
        console_output = Logging.CreateConsoleOutput("console", "INFO")
        Array.Push(context.outputs, console_output)
        
        // Register logger in global registry
        Logging.RegisterLogger(logger)
        
        ReturnValue(logger)
    }
}

Function.Logging.Log {
    Input: (
        logger: Logger,
        level: Text,
        message: Text,
        structured_data: Map[Text, Any] = Map.Create(),
        source_file: Text = "",
        source_line: Integer = 0
    )
    Body: {
        level_enum = Logging.ParseLogLevel(level)
        
        // Early return if level is filtered out
        IfCondition LessThan(level_enum.numeric_value, logger.context.level_filter.numeric_value) ThenBlock {
            ReturnValue()
        }
        
        // Create log entry
        Pool.Logging.Messages.Allocate(entry)
        entry.level = level_enum
        entry.message = message
        entry.structured_data = structured_data
        entry.timestamp_ns = Time.GetUnixTimestampNs()
        entry.thread_id = Thread.GetCurrentId()
        entry.logger_name = logger.name
        entry.source_file = source_file
        entry.source_line = source_line
        
        // Add correlation context if available
        trace_context = Logging.GetCurrentTraceContext()
        IfCondition NotEqual(trace_context, Null) ThenBlock {
            entry.correlation_id = trace_context.trace_id
            entry.trace_id = trace_context.trace_id
            entry.span_id = trace_context.span_id
        }
        
        // Process through filter chain
        should_log = Logging.ApplyFilters(logger.context.filters, entry)
        IfCondition Not(should_log) ThenBlock {
            ReturnValue()
        }
        
        // Update statistics
        Logging.UpdateStats(logger, entry)
        
        // Route to async or sync processing
        IfCondition logger.context.async_enabled ThenBlock {
            Logging.EnqueueAsyncMessage(logger, entry)
        } ElseBlock {
            Logging.ProcessMessageSync(logger, entry)
        }
        
        // Force flush for critical levels
        IfCondition GreaterEqual(level_enum.numeric_value, 40) ThenBlock {  // ERROR and above
            Logging.FlushAll(logger)
        }
    }
}

// Convenience methods for each log level
Function.Logging.Trace {
    Input: (
        logger: Logger,
        message: Text,
        structured_data: Map[Text, Any] = Map.Create(),
        source_file: Text = "",
        source_line: Integer = 0
    )
    Body: {
        Logging.Log(logger, "TRACE", message, structured_data, source_file, source_line)
    }
}

Function.Logging.Debug {
    Input: (
        logger: Logger,
        message: Text,
        structured_data: Map[Text, Any] = Map.Create(),
        source_file: Text = "",
        source_line: Integer = 0
    )
    Body: {
        Logging.Log(logger, "DEBUG", message, structured_data, source_file, source_line)
    }
}

Function.Logging.Info {
    Input: (
        logger: Logger,
        message: Text,
        structured_data: Map[Text, Any] = Map.Create(),
        source_file: Text = "",
        source_line: Integer = 0
    )
    Body: {
        Logging.Log(logger, "INFO", message, structured_data, source_file, source_line)
    }
}

Function.Logging.Warn {
    Input: (
        logger: Logger,
        message: Text,
        structured_data: Map[Text, Any] = Map.Create(),
        source_file: Text = "",
        source_line: Integer = 0
    )
    Body: {
        Logging.Log(logger, "WARN", message, structured_data, source_file, source_line)
    }
}

Function.Logging.Error {
    Input: (
        logger: Logger,
        message: Text,
        structured_data: Map[Text, Any] = Map.Create(),
        source_file: Text = "",
        source_line: Integer = 0
    )
    Body: {
        Logging.Log(logger, "ERROR", message, structured_data, source_file, source_line)
    }
}

Function.Logging.Fatal {
    Input: (
        logger: Logger,
        message: Text,
        structured_data: Map[Text, Any] = Map.Create(),
        source_file: Text = "",
        source_line: Integer = 0
    )
    Body: {
        Logging.Log(logger, "FATAL", message, structured_data, source_file, source_line)
        Logging.FlushAll(logger)  // Always flush fatal messages
    }
}

// ============================================================================
// Structured Logging Extensions
// ============================================================================

Function.Logging.WithFields {
    Input: (
        logger: Logger,
        fields: Map[Text, Any]
    )
    Output: Logger
    Body: {
        // Create a new logger context with additional fields
        Pool.Logging.Messages.Allocate(enriched_logger)
        enriched_logger = Logging.CloneLogger(logger)
        
        // Add fields to all future log entries from this logger
        ForEach field_name, field_value In fields {
            Logging.AddDefaultField(enriched_logger, field_name, field_value)
        }
        
        ReturnValue(enriched_logger)
    }
}

Function.Logging.WithCorrelationId {
    Input: (
        logger: Logger,
        correlation_id: Text
    )
    Output: Logger
    Body: {
        fields = Map.Create()
        Map.Set(fields, "correlation_id", correlation_id)
        ReturnValue(Logging.WithFields(logger, fields))
    }
}

Function.Logging.WithRequestContext {
    Input: (
        logger: Logger,
        request_id: Text,
        user_id: OptionalType[Text] = Null,
        operation: Text = ""
    )
    Output: Logger
    Body: {
        fields = Map.Create()
        Map.Set(fields, "request_id", request_id)
        IfCondition NotEqual(user_id, Null) ThenBlock {
            Map.Set(fields, "user_id", user_id)
        }
        IfCondition NotEqual(operation, "") ThenBlock {
            Map.Set(fields, "operation", operation)
        }
        ReturnValue(Logging.WithFields(logger, fields))
    }
}

// ============================================================================
// Async Processing Engine
// ============================================================================

Function.Logging.EnqueueAsyncMessage {
    Input: (logger: Logger, entry: LogEntry)
    Body: {
        Pool.Logging.AsyncBuffers.Allocate(message)
        message.entry = entry
        message.formatted_message = ""  // Will be formatted in background
        message.target_outputs = Logging.GetOutputNames(logger.context.outputs)
        message.processing_priority = entry.level.numeric_value
        message.batch_id = Logging.GetCurrentBatchId()
        
        // Enqueue in ring buffer with priority ordering
        success = Pool.Logging.AsyncBuffers.EnqueuePriority(message, message.processing_priority)
        
        IfCondition Not(success) ThenBlock {
            // Ring buffer full - either drop or fall back to sync
            IfCondition GreaterEqual(entry.level.numeric_value, 30) ThenBlock {  // WARN and above
                Logging.ProcessMessageSync(logger, entry)  // Important messages go sync
            } ElseBlock {
                Logging.IncrementDroppedCounter(logger)
            }
        }
    }
}

Function.Logging.AsyncProcessorLoop {
    Body: {
        WhileLoop True {
            // Process messages in batches for efficiency
            batch = Pool.Logging.AsyncBuffers.DequeueBatch(max_size-100, timeout_ms-1000)
            
            IfCondition GreaterThan(ArrayLength(batch), 0) ThenBlock {
                Logging.ProcessMessageBatch(batch)
            }
            
            // Yield to prevent CPU hogging
            Thread.Yield()
        }
    }
}

Function.Logging.ProcessMessageBatch {
    Input: (batch: Array[LogMessage])
    Body: {
        // Group messages by output target for efficient batching
        output_groups = Map.Create()
        
        ForEach message In batch {
            ForEach output_name In message.target_outputs {
                IfCondition Not(Map.HasKey(output_groups, output_name)) ThenBlock {
                    Map.Set(output_groups, output_name, Array.Create())
                }
                output_group = Map.Get(output_groups, output_name)
                Array.Push(output_group, message)
            }
        }
        
        // Process each output group
        ForEach output_name, messages In output_groups {
            output = Logging.FindOutput(output_name)
            IfCondition NotEqual(output, Null) ThenBlock {
                Logging.WriteMessagesToOutput(output, messages)
            }
        }
    }
}

// ============================================================================
// Output Writers
// ============================================================================

Function.Logging.WriteMessagesToOutput {
    Input: (output: LogOutput, messages: Array[LogMessage])
    Body: {
        // Format all messages first (can be SIMD optimized)
        formatted_messages = Array.Create()
        ForEach message In messages {
            formatted = Logging.FormatMessage(output.formatter, message.entry)
            Array.Push(formatted_messages, formatted)
        }
        
        // Write based on output type
        Switch output.type {
            Case "console": {
                Logging.WriteToConsole(output, formatted_messages)
            }
            Case "file": {
                Logging.WriteToFile(output, formatted_messages)
            }
            Case "network": {
                Logging.WriteToNetwork(output, formatted_messages)
            }
            Case "journal": {
                Logging.WriteToJournal(output, formatted_messages)
            }
            Case "memory": {
                Logging.WriteToMemoryBuffer(output, formatted_messages)
            }
            DefaultOption: {
                PrintMessage(StringConcat("Unknown output type: ", output.type))
            }
        }
    }
}

Function.Logging.WriteToConsole {
    Input: (output: LogOutput, messages: Array[Text])
    Body: {
        console = output.configuration["handle"]
        
        ForEach message In messages {
            IfCondition console.color_enabled ThenBlock {
                colored_message = Logging.ApplyColorCoding(message, output.formatter)
                Console.WriteLine(colored_message, stream-console.output_stream)
            } ElseBlock {
                Console.WriteLine(message, stream-console.output_stream)
            }
        }
        
        Console.Flush(stream-console.output_stream)
    }
}

Function.Logging.WriteToFile {
    Input: (output: LogOutput, messages: Array[Text])
    Body: {
        file_handle = output.configuration["handle"]
        
        TryBlock: {
            // Check if rotation is needed
            IfCondition Logging.ShouldRotateFile(file_handle) ThenBlock {
                Logging.RotateFile(file_handle)
            }
            
            // Write all messages in one system call for efficiency
            combined_message = StringJoin(messages, "\n")
            File.Write(file_handle.file_descriptor, combined_message)
            
            // Sync based on policy
            file_handle.sync_frequency = Subtract(file_handle.sync_frequency, ArrayLength(messages))
            IfCondition LessEqual(file_handle.sync_frequency, 0) ThenBlock {
                File.Sync(file_handle.file_descriptor)
                file_handle.sync_frequency = 100  // Reset counter
            }
        }
        CatchError.FileWriteError {
            // Fall back to console for critical messages
            console_output = Logging.GetEmergencyConsoleOutput()
            Logging.WriteToConsole(console_output, messages)
        }
    }
}

Function.Logging.WriteToNetwork {
    Input: (output: LogOutput, messages: Array[Text])
    Body: {
        socket_handle = output.configuration["handle"]
        
        TryBlock: {
            // Format messages for network transmission
            ForEach message In messages {
                network_message = IfCondition EqualTo(output.formatter.type, "json")
                    ThenExpression StringConcat(message, "\n")
                    ElseExpression StringConcat("<134>", message, "\n")  // Syslog format
                
                Network.Send(socket_handle, network_message)
            }
        }
        CatchError.NetworkError {
            // Apply retry policy
            success = Logging.RetryNetworkWrite(socket_handle, messages)
            IfCondition Not(success) ThenBlock {
                // Store in local buffer for later retry
                Logging.BufferFailedNetworkMessages(messages)
            }
        }
    }
}

// ============================================================================
// Filtering and Sampling
// ============================================================================

Function.Logging.ApplyFilters {
    Input: (filters: Array[LogFilter], entry: LogEntry)
    Output: Boolean
    Body: {
        ForEach filter In filters {
            should_continue = Logging.ApplyFilter(filter, entry)
            IfCondition Not(should_continue) ThenBlock {
                ReturnValue(False)
            }
        }
        ReturnValue(True)
    }
}

Function.Logging.ApplyFilter {
    Input: (filter: LogFilter, entry: LogEntry)
    Output: Boolean
    Body: {
        Switch filter.type {
            Case "level": {
                min_level = filter.configuration["min_level"]
                ReturnValue(GreaterEqual(entry.level.numeric_value, min_level.numeric_value))
            }
            Case "namespace": {
                allowed_namespaces = filter.configuration["namespaces"]
                namespace = Logging.ExtractNamespace(entry.logger_name)
                ReturnValue(Array.Contains(allowed_namespaces, namespace))
            }
            Case "sampling": {
                sample_rate = filter.configuration["sample_rate"]
                ReturnValue(Logging.ShouldSample(entry, sample_rate))
            }
            Case "rate_limit": {
                max_per_second = filter.configuration["max_per_second"]
                ReturnValue(Logging.CheckRateLimit(entry.logger_name, max_per_second))
            }
            Case "custom": {
                custom_function = filter.filter_function
                ReturnValue(Apply(custom_function, entry))
            }
            DefaultOption: {
                ReturnValue(True)  // Unknown filter type passes through
            }
        }
    }
}

Function.Logging.ShouldSample {
    Input: (entry: LogEntry, sample_rate: FloatingPoint)
    Output: Boolean
    Body: {
        // Use logger name and timestamp for deterministic sampling
        hash_input = StringConcat(entry.logger_name, NumberToString(entry.timestamp_ns))
        hash_value = Hash.FNV1a(hash_input)
        normalized = Divide(Modulo(hash_value, 1000000), 1000000.0)
        ReturnValue(LessEqual(normalized, sample_rate))
    }
}

// ============================================================================
// Performance Monitoring
// ============================================================================

Function.Logging.GetPerformanceMetrics {
    Input: (logger: Logger)
    Output: Map[Text, Any]
    Body: {
        metrics = Map.Create()
        
        Map.Set(metrics, "messages_per_second", Logging.CalculateMessageRate(logger))
        Map.Set(metrics, "cache_efficiency", Logging.CalculateCacheEfficiency(logger))
        Map.Set(metrics, "async_queue_depth", Logging.GetAsyncQueueDepth())
        Map.Set(metrics, "dropped_message_rate", Logging.GetDroppedMessageRate(logger))
        Map.Set(metrics, "memory_usage_bytes", Logging.GetLoggerMemoryUsage(logger))
        Map.Set(metrics, "average_format_time_ns", Logging.GetAverageFormatTime(logger))
        
        ReturnValue(metrics)
    }
}

Function.Logging.StartPerformanceMonitoring {
    Input: (
        interval_seconds: Integer = 60,
        output_to_log: Boolean = True
    )
    Body: {
        monitor_thread = Thread.Create(
            function-Logging.PerformanceMonitorLoop,
            args-[interval_seconds, output_to_log],
            cache_policy-"L3"  // Low priority background task
        )
        Thread.SetName(monitor_thread, "LoggingPerformanceMonitor")
    }
}

Function.Logging.PerformanceMonitorLoop {
    Input: (interval_seconds: Integer, output_to_log: Boolean)
    Body: {
        monitor_logger = Logging.GetLogger("PerformanceMonitor", "INFO", False)
        
        WhileLoop True {
            Thread.Sleep(Multiply(interval_seconds, 1000))
            
            global_metrics = Logging.GetGlobalPerformanceMetrics()
            
            IfCondition output_to_log ThenBlock {
                Logging.Info(monitor_logger, "Logging performance metrics", global_metrics)
            }
            
            // Check for performance issues
            queue_depth = Map.Get(global_metrics, "async_queue_depth")
            IfCondition GreaterThan(queue_depth, 10000) ThenBlock {
                Logging.Warn(monitor_logger, "High async queue depth detected", 
                           Map.FromPairs([["queue_depth", queue_depth]]))
            }
            
            cache_efficiency = Map.Get(global_metrics, "cache_efficiency")
            IfCondition LessThan(cache_efficiency, 0.85) ThenBlock {
                Logging.Warn(monitor_logger, "Low cache efficiency detected",
                           Map.FromPairs([["efficiency", cache_efficiency]]))
            }
        }
    }
}

// ============================================================================
// AI-Friendly Debugging Features
// ============================================================================

Function.Logging.CreateAIDebugLogger {
    Input: (
        component_name: Text,
        trace_ai_decisions: Boolean = True,
        capture_performance: Boolean = True
    )
    Output: Logger
    Body: {
        logger_name = StringConcat("AI.", component_name)
        ai_logger = Logging.GetLogger(logger_name, "DEBUG", True)
        
        // Add AI-specific structured fields
        default_fields = Map.Create()
        Map.Set(default_fields, "component", component_name)
        Map.Set(default_fields, "ai_generated", True)
        Map.Set(default_fields, "trace_decisions", trace_ai_decisions)
        
        enriched_logger = Logging.WithFields(ai_logger, default_fields)
        
        // Add performance monitoring formatter
        IfCondition capture_performance ThenBlock {
            perf_formatter = Logging.CreatePerformanceFormatter()
            Logging.AddFormatter(enriched_logger, perf_formatter)
        }
        
        ReturnValue(enriched_logger)
    }
}

Function.Logging.LogAIDecision {
    Input: (
        logger: Logger,
        decision_point: Text,
        input_data: Any,
        output_decision: Any,
        confidence: FloatingPoint,
        reasoning: Text = ""
    )
    Body: {
        decision_data = Map.Create()
        Map.Set(decision_data, "decision_point", decision_point)
        Map.Set(decision_data, "input_summary", Logging.SummarizeData(input_data))
        Map.Set(decision_data, "decision", output_decision)
        Map.Set(decision_data, "confidence", confidence)
        Map.Set(decision_data, "reasoning", reasoning)
        Map.Set(decision_data, "timestamp", Time.GetISOTimestamp())
        
        Logging.Info(logger, StringConcat("AI Decision: ", decision_point), decision_data)
    }
}

Function.Logging.LogPerformanceOptimization {
    Input: (
        logger: Logger,
        optimization_type: Text,
        before_metrics: Map[Text, Any],
        after_metrics: Map[Text, Any],
        improvement_pct: FloatingPoint
    )
    Body: {
        optimization_data = Map.Create()
        Map.Set(optimization_data, "optimization_type", optimization_type)
        Map.Set(optimization_data, "before", before_metrics)
        Map.Set(optimization_data, "after", after_metrics)
        Map.Set(optimization_data, "improvement_percent", improvement_pct)
        Map.Set(optimization_data, "cache_impact", Logging.CalculateCacheImpact(before_metrics, after_metrics))
        
        level = IfCondition GreaterThan(improvement_pct, 10.0) ThenExpression "INFO" ElseExpression "DEBUG"
        Logging.Log(logger, level, StringConcat("Performance optimization applied: ", optimization_type), optimization_data)
    }
}